{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularized Classification \n",
    "\n",
    "- Wine dataset from the UCI Machine Learning Repository: [data](http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data), [data dictionary](http://archive.ics.uci.edu/ml/datasets/Wine)\n",
    "- **Goal:** Predict the origin of wine using chemical analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and prepare the wine dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0      1     2     3     4    5     6     7     8     9     10    11    12  \\\n",
       "0   1  14.23  1.71  2.43  15.6  127  2.80  3.06  0.28  2.29  5.64  1.04  3.92   \n",
       "1   1  13.20  1.78  2.14  11.2  100  2.65  2.76  0.26  1.28  4.38  1.05  3.40   \n",
       "2   1  13.16  2.36  2.67  18.6  101  2.80  3.24  0.30  2.81  5.68  1.03  3.17   \n",
       "3   1  14.37  1.95  2.50  16.8  113  3.85  3.49  0.24  2.18  7.80  0.86  3.45   \n",
       "4   1  13.24  2.59  2.87  21.0  118  2.80  2.69  0.39  1.82  4.32  1.04  2.93   \n",
       "\n",
       "     13  \n",
       "0  1065  \n",
       "1  1050  \n",
       "2  1185  \n",
       "3  1480  \n",
       "4   735  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the dataset\n",
    "import pandas as pd\n",
    "url = './Datasets/wine.data'\n",
    "wine = pd.read_csv(url, header=None)\n",
    "wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    71\n",
       "1    59\n",
       "3    48\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the response variable\n",
    "wine[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define X and y\n",
    "X = wine.drop(0, axis=1)\n",
    "y = wine[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split into training and testing sets\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression (unregularized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build a logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -3.99122559e-01   7.19280930e-01   8.37692346e-01  -6.15868919e-01\n",
      "   -3.70379019e-02  -2.82429818e-03   1.16591001e+00   5.50064150e-02\n",
      "   -3.01114676e-01  -1.99446284e-01  -9.27763372e-02   9.50747899e-01\n",
      "    1.62914287e-02]\n",
      " [  8.95380409e-01  -1.25706949e+00  -7.99283789e-01   2.31762996e-01\n",
      "    2.24082745e-02   3.58806120e-01   5.15658221e-01   1.70236041e-01\n",
      "    7.77599666e-01  -1.70661033e+00   4.95926028e-01  -2.74442486e-01\n",
      "   -1.39060755e-02]\n",
      " [ -3.48621047e-01   7.05375604e-01   1.11108906e-01   2.02375183e-01\n",
      "   -1.34734684e-02  -6.06889858e-01  -1.85854094e+00  -3.86951461e-02\n",
      "   -7.05962486e-01   1.08803431e+00  -3.93735373e-01  -9.41034622e-01\n",
      "    1.02444280e-03]]\n"
     ]
    }
   ],
   "source": [
    "# examine the coefficients\n",
    "print (logreg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  4.18503497e-03   1.31818492e-02   9.82633116e-01]\n",
      " [  3.84528941e-05   9.98810551e-01   1.15099615e-03]\n",
      " [  9.85235001e-01   1.31000647e-02   1.66493420e-03]\n",
      " [  1.70972460e-02   9.81721849e-01   1.18090522e-03]\n",
      " [  9.88619406e-01   9.44466886e-06   1.13711498e-02]\n",
      " [  2.10450753e-03   2.60259922e-02   9.71869500e-01]\n",
      " [  7.55845820e-02   8.28218255e-01   9.61971626e-02]\n",
      " [  9.98943579e-01   2.74212513e-07   1.05614650e-03]\n",
      " [  5.93269706e-04   1.20002097e-03   9.98206709e-01]\n",
      " [  3.22055476e-04   9.67957409e-01   3.17205351e-02]\n",
      " [  9.92236002e-01   3.54161314e-04   7.40983709e-03]\n",
      " [  1.62195548e-01   8.31946095e-01   5.85835704e-03]\n",
      " [  1.90618519e-04   9.99075621e-01   7.33760182e-04]\n",
      " [  9.98368175e-01   1.31699657e-03   3.14828379e-04]\n",
      " [  1.56512361e-02   9.83719817e-01   6.28947070e-04]\n",
      " [  3.77680447e-04   9.94445742e-01   5.17657781e-03]\n",
      " [  5.03774025e-07   4.12839671e-01   5.87159825e-01]\n",
      " [  9.28633880e-01   6.87946294e-02   2.57149095e-03]\n",
      " [  1.30352993e-04   9.91328499e-01   8.54114832e-03]\n",
      " [  9.83158792e-01   1.28297647e-06   1.68399253e-02]\n",
      " [  9.97686206e-01   1.90239126e-04   2.12355440e-03]\n",
      " [  3.84069071e-04   9.98650883e-01   9.65048342e-04]\n",
      " [  1.94530604e-03   8.31459737e-01   1.66594956e-01]\n",
      " [  1.44637970e-02   9.84385519e-01   1.15068395e-03]\n",
      " [  9.03105184e-01   9.61637320e-02   7.31084325e-04]\n",
      " [  4.17807712e-04   6.28622467e-02   9.36719946e-01]\n",
      " [  9.92958584e-01   6.25722530e-03   7.84191167e-04]\n",
      " [  9.85058383e-01   1.34972457e-02   1.44437151e-03]\n",
      " [  9.91282410e-01   2.03286671e-04   8.51430292e-03]\n",
      " [  4.02454056e-03   9.04172333e-05   9.95885042e-01]\n",
      " [  1.46900316e-02   9.84964513e-01   3.45455725e-04]\n",
      " [  8.76387948e-05   4.06999835e-05   9.99871661e-01]\n",
      " [  2.03384445e-05   1.27742702e-04   9.99851919e-01]\n",
      " [  9.97218732e-01   3.61806473e-06   2.77764997e-03]\n",
      " [  1.76060948e-01   8.22954210e-01   9.84842005e-04]\n",
      " [  1.76684804e-03   9.96690417e-01   1.54273488e-03]\n",
      " [  1.70373690e-04   9.29227303e-01   7.06023234e-02]\n",
      " [  3.26672495e-01   6.70912926e-01   2.41457981e-03]\n",
      " [  3.64153178e-04   9.97084116e-01   2.55173063e-03]\n",
      " [  9.89345722e-01   2.56247186e-03   8.09180608e-03]\n",
      " [  9.94811958e-01   1.94779842e-03   3.24024345e-03]\n",
      " [  5.06018756e-02   9.40251124e-01   9.14700007e-03]\n",
      " [  1.45108089e-04   1.18829139e-02   9.87971978e-01]\n",
      " [  9.71160433e-01   2.81447692e-02   6.94797483e-04]\n",
      " [  9.91294889e-01   6.93247871e-03   1.77263204e-03]]\n"
     ]
    }
   ],
   "source": [
    "# generate predicted probabilities\n",
    "y_pred_prob = logreg.predict_proba(X_test)\n",
    "print (y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.125171555231\n"
     ]
    }
   ],
   "source": [
    "# calculate log loss\n",
    "from sklearn import metrics\n",
    "print (metrics.log_loss(y_test, y_pred_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression (regularized)\n",
    "\n",
    "- [LogisticRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) documentation\n",
    "- **C:** must be positive, decrease for more regularization\n",
    "- **penalty:** l1 (lasso) or l2 (ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# standardize X_train and X_test\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.21034042  0.          0.          0.          0.          0.\n",
      "   0.48870268  0.          0.          0.          0.          0.15214146\n",
      "   1.47740159]\n",
      " [-0.65723336 -0.05608877 -0.11399467  0.          0.          0.          0.\n",
      "   0.          0.         -0.73814175  0.24428926  0.         -0.63398233]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "  -0.84171724  0.          0.          0.61529669 -0.49029377 -0.30484304\n",
      "   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# try C=0.1 with L1 penalty\n",
    "logreg = LogisticRegression(C=0.1, penalty='l1')\n",
    "logreg.fit(X_train_scaled, y_train)\n",
    "print (logreg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.362230079986\n"
     ]
    }
   ],
   "source": [
    "# generate predicted probabilities and calculate log loss\n",
    "y_pred_prob = logreg.predict_proba(X_test_scaled)\n",
    "print (metrics.log_loss(y_test, y_pred_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.59163934  0.06886667  0.33592964 -0.49616684  0.111539    0.21570086\n",
      "   0.40524509 -0.15526139 -0.02534651  0.05399014  0.14877346  0.42327938\n",
      "   0.89815007]\n",
      " [-0.73545676 -0.32942948 -0.47995296  0.294866   -0.1500246   0.04264373\n",
      "   0.14500586  0.07250763  0.17409795 -0.70726652  0.4128986   0.09997212\n",
      "  -0.81284365]\n",
      " [ 0.20136567  0.30989025  0.15977925  0.18867218  0.04204443 -0.27108109\n",
      "  -0.55886639  0.07486943 -0.17471153  0.68266464 -0.52385748 -0.49566967\n",
      "  -0.02565631]]\n"
     ]
    }
   ],
   "source": [
    "# try C=0.1 with L2 penalty\n",
    "logreg = LogisticRegression(C=0.1, penalty='l2')\n",
    "logreg.fit(X_train_scaled, y_train)\n",
    "print (logreg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.244588324539\n"
     ]
    }
   ],
   "source": [
    "# generate predicted probabilities and calculate log loss\n",
    "y_pred_prob = logreg.predict_proba(X_test_scaled)\n",
    "print (metrics.log_loss(y_test, y_pred_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pipeline of StandardScaler and LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "pipe = make_pipeline(StandardScaler(), LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise',\n",
       "       estimator=Pipeline(steps=[('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('logisticregression', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'logisticregression__C': array([  1.00000e-02,   1.00000e-01,   1.00000e+00,   1.00000e+01,\n",
       "         1.00000e+02]), 'logisticregression__penalty': ['l1', 'l2']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='log_loss', verbose=0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# grid search for best combination of C and penalty\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "C_range = 10.**np.arange(-2, 3)\n",
    "penalty_options = ['l1', 'l2']\n",
    "param_grid = dict(logisticregression__C=C_range, logisticregression__penalty=penalty_options)\n",
    "grid = GridSearchCV(pipe, param_grid, cv=10, scoring='log_loss')\n",
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[mean: -1.09861, std: 0.00000, params: {'logisticregression__C': 0.01, 'logisticregression__penalty': 'l1'},\n",
       " mean: -0.62547, std: 0.03037, params: {'logisticregression__C': 0.01, 'logisticregression__penalty': 'l2'},\n",
       " mean: -0.35491, std: 0.06893, params: {'logisticregression__C': 0.10000000000000001, 'logisticregression__penalty': 'l1'},\n",
       " mean: -0.26801, std: 0.04840, params: {'logisticregression__C': 0.10000000000000001, 'logisticregression__penalty': 'l2'},\n",
       " mean: -0.09431, std: 0.06113, params: {'logisticregression__C': 1.0, 'logisticregression__penalty': 'l1'},\n",
       " mean: -0.10371, std: 0.04894, params: {'logisticregression__C': 1.0, 'logisticregression__penalty': 'l2'},\n",
       " mean: -0.05823, std: 0.06375, params: {'logisticregression__C': 10.0, 'logisticregression__penalty': 'l1'},\n",
       " mean: -0.06174, std: 0.05651, params: {'logisticregression__C': 10.0, 'logisticregression__penalty': 'l2'},\n",
       " mean: -0.07046, std: 0.09259, params: {'logisticregression__C': 100.0, 'logisticregression__penalty': 'l1'},\n",
       " mean: -0.06443, std: 0.08409, params: {'logisticregression__C': 100.0, 'logisticregression__penalty': 'l2'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print all log loss scores\n",
    "grid.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0582343588222\n",
      "{'logisticregression__C': 10.0, 'logisticregression__penalty': 'l1'}\n"
     ]
    }
   ],
   "source": [
    "# examine the best model\n",
    "print (grid.best_score_)\n",
    "print (grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing regularized linear models with unregularized linear models\n",
    "\n",
    "**Advantages of regularized linear models:**\n",
    "\n",
    "- Better performance\n",
    "- L1 regularization performs automatic feature selection\n",
    "- Useful for high-dimensional problems (p > n)\n",
    "\n",
    "**Disadvantages of regularized linear models:**\n",
    "\n",
    "- Tuning is required\n",
    "- Feature scaling is recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
